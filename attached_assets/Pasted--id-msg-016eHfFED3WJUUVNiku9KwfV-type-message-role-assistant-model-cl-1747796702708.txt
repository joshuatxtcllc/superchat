{
  "id": "msg_016eHfFED3WJUUVNiku9KwfV",
  "type": "message",
  "role": "assistant",
  "model": "claude-3-7-sonnet-20250219",
  "content": [
    {
      "type": "text",
      "text": "# Multi-Model MCP Interface with Model Selector Chatbot\n\nThis code creates a web interface where users can select from different language models and interact with a chatbot powered by the selected model.\n\n## app.py\n\n```python\nimport os\nimport streamlit as st\nfrom modelhandler import ModelHandler\nfrom chatinterface import ChatInterface\n\ndef main():\n    st.set_page_config(page_title=\"Multi-Model Chat Interface\", layout=\"wide\")\n    \n    st.title(\"Multi-Model MCP Interface\")\n    \n    # Initialize session state\n    if \"model_handler\" not in st.session_state:\n        st.session_state.model_handler = ModelHandler()\n    \n    if \"chat_interface\" not in st.session_state:\n        st.session_state.chat_interface = ChatInterface(st.session_state.model_handler)\n    \n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n    \n    # Create sidebar for model selection\n    with st.sidebar:\n        st.header(\"Model Selection\")\n        available_models = st.session_state.model_handler.get_available_models()\n        \n        selected_model = st.selectbox(\n            \"Choose a model\",\n            available_models,\n            index=0,\n            key=\"model_selector\"\n        )\n        \n        st.session_state.model_handler.set_active_model(selected_model)\n        \n        st.divider()\n        \n        st.header(\"Model Information\")\n        model_info = st.session_state.model_handler.get_model_info(selected_model)\n        \n        st.write(f\"**Name:** {model_info['name']}\")\n        st.write(f\"**Provider:** {model_info['provider']}\")\n        st.write(f\"**Description:** {model_info['description']}\")\n        \n        if st.button(\"Clear Chat History\"):\n            st.session_state.messages = []\n    \n    # Display chat messages\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n    \n    # Handle user input\n    if prompt := st.chat_input(\"What would you like to ask?\"):\n        # Add user message to chat\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n        \n        # Display assistant response\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n            full_response = st.session_state.chat_interface.generate_response(prompt)\n            message_placeholder.markdown(full_response)\n        \n        # Add assistant response to chat history\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## modelhandler.py\n\n```python\nimport os\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\n\nload_dotenv()  # Load environment variables from .env file\n\nclass ModelHandler:\n    def __init__(self):\n        # Initialize API clients\n        self.openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n        self.anthropic_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n        genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n        \n        # Set up model configurations\n        self.models = {\n            \"gpt-4o\": {\n                \"provider\": \"OpenAI\",\n                \"name\": \"GPT-4o\",\n                \"description\": \"OpenAI's multimodal model with strong reasoning capabilities\",\n                \"client\": self.openai_client\n            },\n            \"gpt-3.5-turbo\": {\n                \"provider\": \"OpenAI\",\n                \"name\": \"GPT-3.5 Turbo\",\n                \"description\": \"Efficient and cost-effective OpenAI model\",\n                \"client\": self.openai_client\n            },\n            \"claude-3-opus\": {\n                \"provider\": \"Anthropic\",\n                \"name\": \"Claude 3 Opus\",\n                \"description\": \"Anthropic's most capable model with strong reasoning\",\n                \"client\": self.anthropic_client\n            },\n            \"claude-3-sonnet\": {\n                \"provider\": \"Anthropic\",\n                \"name\": \"Claude 3 Sonnet\",\n                \"description\": \"Balanced performance and speed from Anthropic\",\n                \"client\": self.anthropic_client\n            },\n            \"gemini-pro\": {\n                \"provider\": \"Google\",\n                \"name\": \"Gemini Pro\",\n                \"description\": \"Google's advanced reasoning model\",\n                \"client\": genai\n            }\n        }\n        \n        self.active_model = list(self.models.keys())[0]\n    \n    def get_available_models(self):\n        return list(self.models.keys())\n    \n    def set_active_model(self, model_name):\n        if model_name in self.models:\n            self.active_model = model_name\n            return True\n        return False\n    \n    def get_model_info(self, model_name):\n        if model_name in self.models:\n            return self.models[model_name]\n        return None\n    \n    def get_active_model(self):\n        return self.active_model, self.models[self.active_model]\n\n```\n\n## chatinterface.py\n\n```python\nimport os\n\nclass ChatInterface:\n    def __init__(self, model_handler):\n        self.model_handler = model_handler\n    \n    def generate_response(self, prompt):\n        model_name, model_info = self.model_handler.get_active_model()\n        provider = model_info[\"provider\"]\n        client = model_info[\"client\"]\n        \n        if provider == \"OpenAI\":\n            return self._generate_openai_response(client, model_name, prompt)\n        elif provider == \"Anthropic\":\n            return self._generate_anthropic_response(client, model_name, prompt)\n        elif provider == \"Google\":\n            return self._generate_google_response(client, model_name, prompt)\n        else:\n            return \"Error: Unsupported model provider\"\n    \n    def _generate_openai_response(self, client, model_name, prompt):\n        try:\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.7,\n                max_tokens=1000\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            return f\"Error generating response from {model_name}: {str(e)}\"\n    \n    def _generate_anthropic_response(self, client, model_name, prompt):\n        try:\n            response = client.messages.create(\n                model=model_name,\n                max_tokens=1000,\n                temperature=0.7,\n                system=\"You are a helpful assistant.\",\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n            return response.content[0].text\n        except Exception as e:\n            return f\"Error generating response from {model_name}: {str(e)}\"\n    \n    def _generate_google_response(self, genai, model_name, prompt):\n        try:\n            model = genai.GenerativeModel(model_name)\n            response = model.generate_content(prompt)\n            return response.text\n        except Exception as e:\n            return f\"Error generating response from {model_name}: {str(e)}\"\n```\n\n## requirements.txt\n\n```\nstreamlit>=1.27.0\nopenai>=1.1.1\nanthropic>=0.5.0\ngoogle-generativeai>=0.2.0\npython-dotenv>=1.0.0\n```\n\n## .env (create this file with your API keys)\n\n```\nOPENAI_API_KEY=your_openai_api_key_here\nANTHROPIC_API_KEY=your_anthropic_api_key_here\nGOOGLE_API_KEY=your_google_api_key_here\n```\n\n## How to Run the Application\n\n1. Clone the repository or create the files as shown above\n2. Create a virtual environment: `python -m venv venv`\n3. Activate the virtual environment:\n   - Windows: `venv\\Scripts\\activate`\n   - macOS/Linux: `source venv/bin/activate`\n4. Install the requirements: `pip install -r requirements.txt`\n5. Create a `.env` file with your API keys\n6. Run the Streamlit app: `streamlit run app.py`\n\n## Features\n\n- Multi-model interface supporting OpenAI, Anthropic, and Google models\n- Easy model switching through the sidebar\n- Chat interface with message history\n- Information display about the currently selected model\n- Clear chat history option\n\nThis implementation creates a clean, user-friendly interface where users can easily switch between different LLM providers and models while maintaining a conversation thread."
    }
  ],
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 22,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "output_tokens": 2288
  }
}